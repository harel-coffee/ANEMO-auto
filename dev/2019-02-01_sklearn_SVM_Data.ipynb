{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - utiliser juste deux classes : keep / reject\n",
    " - multi-variate X en fonction du dictionaire incluant list_keys_param_fit et goodness_of_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn import svm, datasets\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 1- Split into a training set and a test set using a ShuffleSplit + doing that in parallel for the differrent features to test\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "def split_data(X, y, test_size=.25, train_size=None) :\n",
    "    \n",
    "\n",
    "    rs = ShuffleSplit(n_splits=1, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    for index_train, index_test in rs.split(y): pass\n",
    "\n",
    "    X_train, X_test = {}, {}\n",
    "    X_train, X_test = X[index_train, :], X[index_test, :]\n",
    "    y_train, y_test =  y[index_train].copy(), y[index_test].copy()\n",
    "\n",
    "    print('X: nb_trial_train : %s, nb_trial_test : %s'%(X_train.shape[0], X_test.shape[0]))\n",
    "    print('y: nb_trial_train : %s, nb_trial_test : %s'%(y_train.shape[0], y_test.shape[0]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 4- Train a SVM classification model\n",
    "#------------------------------------------------------------------------------\n",
    "# When training an SVM with the Radial Basis Function (RBF) kernel,\n",
    "#    two parameters must be considered: C and gamma.\n",
    "# The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface.\n",
    "#   A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly.\n",
    "# Gamma defines how much influence a single training example has.\n",
    "#   The larger gamma is, the closer other examples must be to be affected.\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV # see http://scikit-learn.org/stable/modules/grid_search.html\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def Train_SVM(X_train, y_train, size_c_gamma=32, ax=None, fig=None, plot=None) :\n",
    "\n",
    "\n",
    "    C_range = np.logspace(-5, 10., size_c_gamma, base=2.)\n",
    "    gamma_range = np.logspace(-14, 3, size_c_gamma, base=2.)\n",
    "\n",
    "    \n",
    "\n",
    "    '''param_grid = [{'kernel': ['rbf', 'poly', 'sigmoid'], 'gamma': gamma_range, 'C': C_range},\n",
    "                  {'kernel': ['linear'], 'C': C_range}]'''\n",
    "    \n",
    "    liste_kernel = ['rbf']#, 'poly', 'sigmoid']\n",
    "    param_grid = [{'kernel': liste_kernel, 'gamma': gamma_range, 'C': C_range}]\n",
    "\n",
    "    \n",
    "    grid = GridSearchCV(SVC(verbose=False, tol=1e-3, max_iter = -1, ),\n",
    "                        param_grid, verbose=0, scoring='f1_weighted', n_jobs=1,) #cv=50, \n",
    "\n",
    "    X_train_ = np.zeros((len(X_train), 0))\n",
    "    X_train_ = np.hstack((X_train_, X_train))\n",
    "    grid.fit(X_train, y_train.ravel())\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# 5- Quantitative evaluation of the model quality on the test set\n",
    "#------------------------------------------------------------------------------\n",
    "def Quantitative_evaluation(grid, X_test, classes, ax=None, fig=None, plot=None) :\n",
    "\n",
    "    import itertools\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    \n",
    "    y_pred = grid.predict(X_test)\n",
    "    fone_score = np.array(metrics.f1_score(y_test, y_pred, average=None)).mean()\n",
    "    \n",
    "    print('Confusion matrix\\n(fone_score on test => Accuracy = %0.2f)' % (fone_score))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from ANEMO import read_edf\n",
    "from ANEMO import ANEMO\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sujet = ['AM','BMC','CS','DC','FM','IP','LB','OP','RS','SR','TN','YK']\n",
    "time = ['2017-10-23_100057','2017-09-26_095637', '2017-10-03_134421','2017-09-27_161040',\n",
    "        '2017-10-03_143803','2017-09-28_115250', '2017-09-20_151043','2017-10-26_121823',\n",
    "        '2017-11-08_094717','2017-11-16_153313', '2017-11-08_150410','2017-11-17_172706']\n",
    "file = os.path.join('parametre', 'Delete_list_trials_velocity_fct.pkl')\n",
    "with open(file, 'rb') as fichier :\n",
    "    Delete_list_trials = pickle.load(fichier, encoding='latin1')\n",
    "\n",
    "file = os.path.join('parametre', 'Delete_list_Bad_trials_velocity.pkl')\n",
    "with open(file, 'rb') as fichier :\n",
    "    Bad_Fit = pickle.load(fichier, encoding='latin1')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nb_nan - residu = 0.69\n",
    "nb_nan - maxi = 0.67\n",
    "nb_nan - redchi = 0.67\n",
    "nb_nan - latence = 0.64\n",
    "nb_nan - v_anti = 0.63\n",
    "nb_nan - start_anti = 0.62\n",
    "\n",
    "-----------------------------\n",
    "nb_nan - tau = 0.57\n",
    "nb_nan - nfev = 0.56\n",
    "residu - start_anti = 0.55\n",
    "nb_nan - chisqr = 0.51\n",
    "residu - chisqr = 0.51\n",
    "residu - redchi = 0.51\n",
    "\n",
    "nb_nan - aic = 0.48\n",
    "nb_nan - bic = 0.48\n",
    "residu - nfev = 0.48\n",
    "residu - aic = 0.48\n",
    "residu - bic = 0.48"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "aic - maxi = 0.58\n",
    "--------------------------------------------\n",
    "\n",
    "nb_nan - tau = 0.57\n",
    "residu - latence = 0.57\n",
    "\n",
    "nb_nan - nfev = 0.56\n",
    "nfev - maxi = 0.56\n",
    "aic - latence = 0.56\n",
    "\n",
    "residu - start_anti = 0.55\n",
    "\n",
    "aic - redchi = 0.54\n",
    "\n",
    "residu - tau = 0.52\n",
    "\n",
    "nb_nan - chisqr = 0.51\n",
    "residu - chisqr = 0.51\n",
    "residu - redchi = 0.51\n",
    "chisqr - redchi = 0.51\n",
    "chisqr - maxi = 0.51\n",
    "nfev - latence = 0.51\n",
    "aic - start_anti = 0.51\n",
    "\n",
    "residu - v_anti = 0.50\n",
    "chisqr - nfev = 0.50\n",
    "chisqr - v_anti = 0.50\n",
    "chisqr - latence = 0.50\n",
    "nfev - aic = 0.50\n",
    "bic - redchi = 0.50\n",
    "bic - start_anti = 0.50\n",
    "\n",
    "nfev - start_anti = 0.49\n",
    "nfev - bic = 0.49\n",
    "aic - tau = 0.49\n",
    "bic - tau = 0.49\n",
    "\n",
    "nb_nan - aic = 0.48\n",
    "nb_nan - bic = 0.48\n",
    "residu - nfev = 0.48\n",
    "residu - aic = 0.48\n",
    "residu - bic = 0.48\n",
    "chisqr - aic = 0.48\n",
    "chisqr - bic = 0.48\n",
    "chisqr - start_anti = 0.48\n",
    "chisqr - tau = 0.48\n",
    "nfev - redchi = 0.48\n",
    "nfev - tau = 0.48\n",
    "nfev - v_anti = 0.48\n",
    "aic - bic = 0.48\n",
    "aic - v_anti = 0.48"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "liste = ['nb_nan', 'residu', 'bic', 'redchi', 'chisqr', 'nfev', 'aic', 'start_anti', 'maxi', 'tau', 'latence', 'v_anti']\n",
    "l = [[], [], [], [], [], [], [], [], []]\n",
    "\n",
    "a = [[[[[[[[[[[[\n",
    "    l[0].append([liste[a], liste[b], liste[c], liste[d],\n",
    "              liste[e], liste[f], liste[g], liste[h],\n",
    "              liste[i], liste[j], liste[k]])\n",
    "    for k in range(j+1, len(liste))] for j in range(i+1, len(liste))]\n",
    "    for i in range(h+1, len(liste))] for h in range(g+1, len(liste))]\n",
    "    for g in range(f+1, len(liste))] for f in range(e+1, len(liste))]\n",
    "    for e in range(d+1, len(liste))] for d in range(c+1, len(liste))]\n",
    "    for c in range(b+1, len(liste))] for b in range(a+1, len(liste))]\n",
    "    for a in range(len(liste))]]\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "nb_nan - residu = 0.69\n",
    "nb_nan - redchi = 0.67\n",
    "nb_nan - maxi = 0.67\n",
    "--------------------------------------------\n",
    "['nb_nan', 'residu', 'redchi', 'maxi'] - 0.68\n",
    "['nb_nan', 'residu', 'redchi'] - 0.66\n",
    "['nb_nan', 'residu', 'maxi'] - 0.69\n",
    "['nb_nan', 'redchi', 'maxi'] - 0.68\n",
    "['residu', 'redchi', 'maxi'] - 0.61\n",
    "--------------------------------------------\n",
    "nb_nan - latence = 0.64\n",
    "--------------------------------------------\n",
    "['nb_nan', 'residu', 'redchi', 'maxi', 'latence'] - 0.63\n",
    "\n",
    "['nb_nan', 'residu', 'redchi', 'latence']  - 0.61\n",
    "['nb_nan', 'residu', 'maxi', 'latence'] - 0.68\n",
    "['nb_nan', 'residu', 'latence'] - 0.62\n",
    "\n",
    "['nb_nan', 'redchi', 'maxi', 'latence']\n",
    "['nb_nan', 'redchi', 'latence']\n",
    "['nb_nan', 'maxi', 'latence']\n",
    "\n",
    "['residu', 'redchi', 'maxi', 'latence']\n",
    "['residu', 'redchi', 'latence']\n",
    "['residu', 'maxi', 'latence']\n",
    "\n",
    "['redchi', 'maxi', 'latence']\n",
    "\n",
    "--------------------------------------------\n",
    "nb_nan - v_anti = 0.63\n",
    "---------------------------------------------\n",
    "['nb_nan', 'residu', 'redchi', 'maxi', 'latence', 'v_anti'] - 0.65\n",
    "\n",
    "['nb_nan', 'residu', 'redchi', 'maxi', 'v_anti']  - 0.69\n",
    "['nb_nan', 'residu', 'redchi', 'latence', 'v_anti']\n",
    "['nb_nan', 'residu', 'maxi', 'latence', 'v_anti']   - 0.67\n",
    "['nb_nan', 'residu', 'redchi', 'v_anti']\n",
    "['nb_nan', 'residu', 'maxi', 'v_anti']\n",
    "['nb_nan', 'residu', 'latence', 'v_anti']\n",
    "['nb_nan', 'residu', 'v_anti']\n",
    "\n",
    "['nb_nan', 'redchi', 'maxi', 'latence', 'v_anti']\n",
    "['nb_nan', 'redchi', 'maxi', 'v_anti']\n",
    "['nb_nan', 'redchi', 'latence', 'v_anti']\n",
    "['nb_nan', 'redchi', 'v_anti']\n",
    "['nb_nan', 'maxi', 'latence', 'v_anti']\n",
    "['nb_nan', 'maxi', 'v_anti']\n",
    "['nb_nan', 'latence', 'v_anti']\n",
    "\n",
    "['residu', 'redchi', 'maxi', 'latence', 'v_anti']\n",
    "['residu', 'redchi', 'maxi', 'v_anti']\n",
    "['residu', 'redchi', 'latence', 'v_anti']\n",
    "['residu', 'maxi', 'latence', 'v_anti']\n",
    "['residu', 'maxi', 'v_anti']\n",
    "['residu', 'latence', 'v_anti']\n",
    "\n",
    "['redchi', 'maxi', 'latence', 'v_anti']\n",
    "['redchi', 'maxi', 'v_anti']\n",
    "['redchi', 'latence', 'v_anti']\n",
    "\n",
    "['maxi', 'latence', 'v_anti']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "residu - maxi = 0.63\n",
    "nb_nan - start_anti = 0.62\n",
    "--------------------------------------------\n",
    "\n",
    "['nb_nan', 'residu', 'v_anti', 'maxi', 'latence', 'start_anti'] - 0.64\n",
    "['nb_nan', 'residu', 'redchi', 'v_anti', 'maxi', 'latence', 'start_anti'] - 0.63\n",
    "['nb_nan', 'residu', 'maxi', 'latence', 'start_anti'] - 0.62\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['nb_nan', 'redchi', 'maxi', 'latence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AM -- nb_Delete_trials = [0, 1, 0], nb_Bad_Fit = [1, 0, 0]\n",
      "BMC -- nb_Delete_trials = [2, 7, 2], nb_Bad_Fit = [0, 1, 3]\n",
      "CS -- nb_Delete_trials = [0, 0, 2], nb_Bad_Fit = [3, 16, 10]\n",
      "DC -- nb_Delete_trials = [3, 7, 2], nb_Bad_Fit = [6, 7, 6]\n",
      "FM -- nb_Delete_trials = [22, 11, 3], nb_Bad_Fit = [3, 3, 3]\n",
      "IP -- nb_Delete_trials = [0, 1, 0], nb_Bad_Fit = [0, 0, 0]\n",
      "LB -- nb_Delete_trials = [10, 7, 7], nb_Bad_Fit = [12, 11, 6]\n",
      "OP -- nb_Delete_trials = [29, 19, 22], nb_Bad_Fit = [16, 29, 32]\n",
      "RS -- nb_Delete_trials = [0, 2, 5], nb_Bad_Fit = [1, 7, 1]\n",
      "SR -- nb_Delete_trials = [8, 2, 2], nb_Bad_Fit = [5, 23, 5]\n",
      "TN -- nb_Delete_trials = [7, 3, 5], nb_Bad_Fit = [11, 7, 4]\n",
      "YK -- nb_Delete_trials = [66, 35, 22], nb_Bad_Fit = [64, 75, 66]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#list_keys_param_fit = ['start_anti', 'tau', 'v_anti', 'latence', 'maxi']\n",
    "list_keys_param_fit = ['maxi', 'latence']\n",
    "X, y, var = [], [], []\n",
    "a = 0\n",
    "for x in range(len(sujet)) :\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    file = os.path.join('data', 'enregistrement_' + sujet[x] + '_' + time[x] + '.pkl')\n",
    "    with open(file, 'rb') as fichier :\n",
    "        param_exp = pickle.load(fichier, encoding='latin1')\n",
    "    A = ANEMO(param_exp)\n",
    "    N_trials, N_blocks = param_exp['N_trials'], param_exp['N_blocks']\n",
    "\n",
    "    resultats = os.path.join('data', 'enregistrement_' + sujet[x] + '_' + time[x] + '.asc')\n",
    "    data = read_edf(resultats, 'TRIALID')\n",
    "\n",
    "    file = os.path.join('parametre/goodness_of_fit_'+ sujet[x] +'.pkl')\n",
    "    with open(file, 'rb') as fichier :\n",
    "        goodness_of_fit = pickle.load(fichier, encoding='latin1')\n",
    "    \n",
    "    file = os.path.join('parametre/param_Fit_'+ sujet[x] +'_fct_velocity_2_step_False_whitening.pkl')\n",
    "    with open(file, 'rb') as fichier :\n",
    "        param_fit = pickle.load(fichier, encoding='latin1')\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    nb_del_trial = [len(Delete_list_trials[sujet[x]][b]) for b in range(N_blocks)]\n",
    "    nb_bad_trial = [len(Bad_Fit[sujet[x]][b]) for b in range(N_blocks)]\n",
    "    print('%s -- nb_Delete_trials = %s, nb_Bad_Fit = %s'%(sujet[x], nb_del_trial, nb_bad_trial))\n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    for block in range(N_blocks) :\n",
    "        for trial in range(N_trials) :\n",
    "\n",
    "            if trial in Delete_list_trials[sujet[x]][block] : y.append(1)\n",
    "            elif trial in Bad_Fit[sujet[x]][block] : y.append(1)\n",
    "            else : y.append(0)\n",
    "\n",
    "            if a==0 :\n",
    "                var.append('nb_nan')\n",
    "                X.append([])\n",
    "                '''var.append('residu')\n",
    "                X.append([])'''\n",
    "                for key in goodness_of_fit.keys() :\n",
    "                    if key in ['redchi'] :\n",
    "                        var.append(key)\n",
    "                        X.append([])\n",
    "                for key in param_fit.keys() :\n",
    "                    if key in list_keys_param_fit :\n",
    "                        var.append(key)\n",
    "                        X.append([])\n",
    "                a=1\n",
    "            \n",
    "            trial_data = trial + N_trials*block\n",
    "            arg = A.arg(data[trial_data], trial=trial, block=block)\n",
    "            velocity_NAN = A.velocity_NAN(**arg)[0]\n",
    "\n",
    "            \n",
    "            nb_nan = [x for x in velocity_NAN[arg.StimulusOf-arg.t_0:] if str(x)=='nan']\n",
    "            X[0].append(len(nb_nan)/len(velocity_NAN[arg.StimulusOf-arg.t_0:]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''t1 , res = 0, np.zeros(len(velocity_NAN)-280)\n",
    "            for t in range(len(velocity_NAN)-280) :\n",
    "                if np.isnan(velocity_NAN[t]) :\n",
    "                    res[t] = np.nan\n",
    "                else :\n",
    "                    res[t] = abs(goodness_of_fit['residual'][block][trial][t1])\n",
    "                    t1 = t1+1\n",
    "            residu = np.nanmean(res[arg.StimulusOf-arg.t_0:])\n",
    "            \n",
    "            X[1].append(residu)'''\n",
    "            \n",
    "            k = 1\n",
    "            for key in goodness_of_fit.keys() :\n",
    "                if key in ['redchi'] :\n",
    "                    X[k].append(goodness_of_fit[key][block][trial])\n",
    "                    k=k+1\n",
    "    \n",
    "            for key in param_fit.keys() :\n",
    "                if key in list_keys_param_fit :\n",
    "                    X[k].append(param_fit[key][block][trial])\n",
    "                    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nb_nan', 'redchi', 'maxi', 'latence']\n",
      "X: nb_trial_train : 2160, nb_trial_test : 5040\n",
      "y: nb_trial_train : 2160, nb_trial_test : 5040\n",
      "nb_Bad_trial in y_train : 184 ; y_test : 431\n",
      "Confusion matrix\n",
      "(fone_score on test => Accuracy = 0.63)\n"
     ]
    }
   ],
   "source": [
    "print(var)\n",
    "\n",
    "classes = ['keep', 'reject']\n",
    "\n",
    "X1 = np.array(X) ; y1 = np.array(y)\n",
    "X1 = X1.transpose()\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X1, y1, test_size=.7, train_size=None)\n",
    "print('nb_Bad_trial in y_train : %s ; y_test : %s'%(len(y_train[y_train>0]), len(y_test[y_test>0])))\n",
    "\n",
    "\n",
    "X_fit = X_train ; X_t = X_test\n",
    "grid = Train_SVM(X_fit, y_train, size_c_gamma=20, ax=None, fig=None, plot=None) #size_c_gamma=32\n",
    "y_pred = Quantitative_evaluation(grid, X_t, classes, ax=None, fig=None, plot=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = load_digits(return_X_y=True)\n",
    "# Throw away data, to be in the curse of dimension settings\n",
    "X = X[:200]\n",
    "y = y[:200]\n",
    "n_samples = len(y)\n",
    "X = X.reshape((n_samples, -1))\n",
    "# add 200 non-informative features\n",
    "X = np.hstack((X, 2 * np.random.random((n_samples, 200))))\n",
    "print(np.shape(X), np.shape(y))\n",
    "# #############################################################################\n",
    "# Create a feature-selection transform and an instance of SVM that we\n",
    "# combine together to have an full-blown estimator\n",
    "\n",
    "transform = SelectPercentile(chi2)\n",
    "\n",
    "clf = Pipeline([('anova', transform), ('svc', SVC(gamma=\"auto\"))])\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    # Compute cross-validation score using 1 CPU\n",
    "    this_scores = cross_val_score(clf, X, y, cv=5, n_jobs=1)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "\n",
    "plt.title('Performance of the SVM-Anova varying the percentile of features selected')\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Prediction rate')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
